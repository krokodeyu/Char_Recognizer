{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import shutil\n",
    "from glob import glob\n",
    "import json\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# 下载数据集\n",
    "links = pd.read_csv('./mchar_data_list_0515.csv')    #修改成你电脑对应的路径\n",
    "dataset_path = \"./dataset\"\n",
    "print(f\"数据集目录：{dataset_path}\")\n",
    "if not os.path.exists(dataset_path):\n",
    "    os.mkdir(dataset_path)\n",
    "for i,link in enumerate(links['link']):\n",
    "    file_name = links['file'][i]\n",
    "    print(file_name, '\\t', link)\n",
    "    file_name = os.path.join(dataset_path,file_name)\n",
    "    if not os.path.exists(file_name):\n",
    "        response = requests.get(link, stream=True)\n",
    "        with open(file_name, 'wb') as f:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "zip_list = ['mchar_train', 'mchar_test_a', 'mchar_val']\n",
    "for little_zip in zip_list:\n",
    "    zip_name = os.path.join(dataset_path,little_zip)\n",
    "    if not os.path.exists(zip_name):\n",
    "        zip_file = zipfile.ZipFile(os.path.join(dataset_path,f\"{little_zip}.zip\"), 'r')\n",
    "        zip_file.extractall(path = dataset_path)\n",
    "# if os.path.exists(os.path.join(dataset_path,'__MACOSX')):\n",
    "#     shutil.rmtree(os.path.join(dataset_path,'__MACOSX'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建数据集路径索引\n",
    "data_dir = {\n",
    "    'train_data': f'{dataset_path}/mchar_train/',\n",
    "    'val_data': f'{dataset_path}/mchar_val/',\n",
    "    'test_data': f'{dataset_path}/mchar_test_a/',\n",
    "    'train_label': f'{dataset_path}/mchar_train.json',\n",
    "    'val_label': f'{dataset_path}/mchar_val.json',\n",
    "    'submit_file': f'{dataset_path}/mchar_sample_submit_A.csv'\n",
    "}\n",
    "\n",
    "train_list = glob(data_dir['train_data']+'*.png')\n",
    "test_list = glob(data_dir['test_data']+'*.png')\n",
    "val_list = glob(data_dir['val_data']+'*.png')\n",
    "print('train image counts: %d'%len(train_list))\n",
    "print('val image counts: %d'%len(val_list))\n",
    "print('test image counts: %d'%len(test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 看train数据集第一张的信息，长宽高等\n",
    "def look_train_json():\n",
    "    with open(data_dir['train_label'], 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    content = json.loads(content)\n",
    "    print(content['000000.png'])\n",
    "\n",
    "look_train_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 看需要输出文件的信息\n",
    "def look_submit():\n",
    "    df = pd.read_csv(data_dir['submit_file'], sep=',')\n",
    "    print(df.head(5))\n",
    "\n",
    "look_submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 统计图片的大小\n",
    "def img_size_summary():\n",
    "    sizes = []\n",
    "\n",
    "    for img in glob(data_dir['train_data']+'*.png'):\n",
    "        img = Image.open(img)\n",
    "\n",
    "        sizes.append(img.size)\n",
    "\n",
    "    sizes = np.array(sizes)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(sizes[:, 0], sizes[:, 1])\n",
    "    plt.xlabel('Width')\n",
    "    plt.ylabel('Height')\n",
    "\n",
    "    plt.title('image width-height summary')\n",
    "    # 保存图像到文件\n",
    "    plt.show()\n",
    "\n",
    "img_size_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#统计bbox的大小\n",
    "def bbox_summary():\n",
    "    marks = json.loads(open(data_dir['train_label'], 'r').read())\n",
    "    bboxes = []\n",
    "\n",
    "    for img, mark in marks.items():\n",
    "        for i in range(len(mark['label'])):\n",
    "            bboxes.append([mark['left'][i], mark['top'][i], mark['width'][i], mark['height'][i]])\n",
    "\n",
    "    bboxes = np.array(bboxes)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    ax.scatter(bboxes[:, 2], bboxes[:, 3])\n",
    "    ax.set_title('bbox width-height summary')\n",
    "    ax.set_xlabel('width')\n",
    "    ax.set_ylabel('height')\n",
    "    # 保存图像到文件\n",
    "    plt.show()\n",
    "\n",
    "bbox_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#统计图片分别含有数字的个数\n",
    "def label_summary():\n",
    "    marks = json.load(open(data_dir['train_label'], 'r'))\n",
    "    dicts = {}\n",
    "    for img, mark in marks.items():\n",
    "        if len(mark['label']) not in dicts:\n",
    "            dicts[len(mark['label'])] = 0\n",
    "        dicts[len(mark['label'])] += 1\n",
    "\n",
    "    dicts = sorted(dicts.items(), key=lambda x: x[0])\n",
    "    for k, v in dicts:\n",
    "        print('%d个数字的图片数目: %d' % (k, v))\n",
    "\n",
    "label_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from glob import glob\n",
    "import torch as t\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, MultiStepLR\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patch\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "from torchvision.models.mobilenet import mobilenet_v2\n",
    "from torchvision.models.resnet import resnet50, resnet34,resnet18,resnet101\n",
    "from torchsummary import summary\n",
    "import random\n",
    "from torchvision.models.mobilenet import MobileNetV2\n",
    "from torch.optim import Adam\n",
    "#超参数设定\n",
    "class Config:\n",
    "    batch_size = 64\n",
    "    lr = 1e-3\n",
    "    momentum = 0.9\n",
    "    weights_decay = 1e-4\n",
    "    class_num = 11\n",
    "    eval_interval = 1\n",
    "    checkpoint_interval = 5\n",
    "    print_interval = 50\n",
    "    checkpoints = '/data/duyongkun/CPX/OCR_competition/crnn/checkpoints'   # 自己创建一个文件夹用来储存权重\n",
    "    pretrained = None \n",
    "    start_epoch = 0\n",
    "    epoches = 30\n",
    "    smooth = 0.1\n",
    "    erase_prob = 0.5\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DigitsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    DigitsDataset\n",
    "    Params:\n",
    "      data_dir(string): data directory\n",
    "      label_path(string): label path\n",
    "      aug(bool): wheather do image augmentation, default: True\n",
    "    \"\"\"\n",
    "    def __init__(self, mode='train', size=(128, 256), aug=True):\n",
    "        super(DigitsDataset, self).__init__()\n",
    "        self.aug = aug\n",
    "        self.size = size\n",
    "        self.mode = mode\n",
    "        self.width = 224\n",
    "        self.batch_count = 0\n",
    "        if mode == 'test':\n",
    "            self.imgs = glob(data_dir['test_data'] + '*.png')\n",
    "            self.labels = None\n",
    "        else:\n",
    "            labels = json.load(open(data_dir['%s_label' % mode], 'r'))\n",
    "            imgs = glob(data_dir['%s_data' % mode] + '*.png')\n",
    "            self.imgs = [(img, labels[os.path.split(img)[-1]]) for img in imgs \\\n",
    "                         if os.path.split(img)[-1] in labels]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode != 'test':\n",
    "            img, label = self.imgs[idx]\n",
    "        else:\n",
    "            img = self.imgs[idx]\n",
    "            label = None\n",
    "        img = Image.open(img)\n",
    "        trans0 = [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ]\n",
    "        min_size = self.size[0] if (img.size[1] / self.size[0]) < ((img.size[0] / self.size[1])) else self.size[1]\n",
    "        trans1 = [\n",
    "            transforms.Resize(128),\n",
    "            transforms.CenterCrop((128, self.width))\n",
    "        ]\n",
    "        if self.aug:\n",
    "            trans1.extend([\n",
    "                transforms.ColorJitter(0.1, 0.1, 0.1),\n",
    "                transforms.RandomGrayscale(0.1),\n",
    "                transforms.RandomAffine(15, translate=(0.05, 0.1), shear=5)\n",
    "            ])\n",
    "        trans1.extend(trans0)\n",
    "        if self.mode != 'test':\n",
    "            return transforms.Compose(trans1)(img), t.tensor(\n",
    "                label['label'][:4] + (4 - len(label['label'])) * [10]).long()\n",
    "        else:\n",
    "            # trans1.append(transforms.RandomErasing(scale=(0.02, 0.1)))\n",
    "            return transforms.Compose(trans1)(img), self.imgs[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def collect_fn(self, batch):\n",
    "        imgs, labels = zip(*batch)\n",
    "        if self.mode == 'train':\n",
    "            if self.batch_count > 0 and self.batch_count % 10 == 0:\n",
    "                self.width = random.choice(range(224, 256, 16))\n",
    "\n",
    "        self.batch_count += 1\n",
    "        return t.stack(imgs).float(), t.stack(labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitsResnet50(nn.Module):\n",
    "    def __init__(self, class_num=11):\n",
    "        super(DigitsResnet50, self).__init__()\n",
    "        self.net = resnet50(pretrained=True)\n",
    "        self.net = nn.Sequential(*list(self.net.children())[:-1]) \n",
    "        self.cnn = self.net\n",
    "        self.fc1 = nn.Linear(2048, class_num)\n",
    "        self.fc2 = nn.Linear(2048, class_num)\n",
    "        self.fc3 = nn.Linear(2048, class_num)\n",
    "        self.fc4 = nn.Linear(2048, class_num)\n",
    "\n",
    "    def forward(self, img):\n",
    "        feat = self.cnn(img)\n",
    "        feat = feat.view(feat.shape[0], -1)\n",
    "        c1 = self.fc1(feat)\n",
    "        c2 = self.fc2(feat)\n",
    "        c3 = self.fc3(feat)\n",
    "        c4 = self.fc4(feat)\n",
    "        return c1, c2, c3, c4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothEntropy(nn.Module):\n",
    "    def __init__(self, smooth=0.1, class_weights=None, size_average='mean'):\n",
    "        super(LabelSmoothEntropy, self).__init__()\n",
    "        self.size_average = size_average\n",
    "        self.smooth = smooth\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        lb_pos, lb_neg = 1 - self.smooth, self.smooth / (preds.shape[0] - 1)\n",
    "        smoothed_lb = t.zeros_like(preds).fill_(lb_neg).scatter_(1, targets[:, None], lb_pos)\n",
    "        log_soft = F.log_softmax(preds, dim=1)\n",
    "        if self.class_weights is not None:\n",
    "            loss = -log_soft * smoothed_lb * self.class_weights[None, :]\n",
    "        else:\n",
    "            loss = -log_soft * smoothed_lb\n",
    "        loss = loss.sum(1)\n",
    "        if self.size_average == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.size_average == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, val=True):\n",
    "        self.device = t.device('cuda') if t.cuda.is_available() else t.device('cpu')\n",
    "        self.train_set = DigitsDataset(mode='train')\n",
    "        self.train_loader = DataLoader(self.train_set, batch_size=config.batch_size, shuffle=True, num_workers=8,\n",
    "                                       pin_memory=True, \\\n",
    "                                       drop_last=True, collate_fn=self.train_set.collect_fn)\n",
    "        if val:\n",
    "            self.val_loader = DataLoader(DigitsDataset(mode='val', aug=False), batch_size=config.batch_size, \\\n",
    "                                         num_workers=8, pin_memory=True, drop_last=False)\n",
    "        else:\n",
    "            self.val_loader = None\n",
    "\n",
    "        self.model = DigitsResnet50(config.class_num).to(self.device)\n",
    "        self.criterion = LabelSmoothEntropy().to(self.device)\n",
    "        self.optimizer = Adam(self.model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0,\n",
    "                              amsgrad=False)\n",
    "        self.lr_scheduler = CosineAnnealingWarmRestarts(self.optimizer, T_0=10, T_mult=2, eta_min=0)\n",
    "        self.best_acc = 0\n",
    "        self.best_checkpoint_path = \"\"\n",
    "        # 是否载入预训练模型\n",
    "        if config.pretrained is not None:\n",
    "            self.load_model(config.pretrained)\n",
    "            # print('Load model from %s'%config.pretrained)\n",
    "            if self.val_loader is not None:\n",
    "                acc = self.eval()\n",
    "            self.best_acc = acc\n",
    "            print('Load model from %s, Eval Acc: %.2f' % (config.pretrained, acc * 100))\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(config.start_epoch, config.epoches):\n",
    "            acc = self.train_epoch(epoch)\n",
    "            if (epoch + 1) % config.eval_interval == 0:\n",
    "                print('Start Evaluation')\n",
    "                if self.val_loader is not None:\n",
    "                    acc = self.eval()\n",
    "                #保存最优模型\n",
    "                if acc > self.best_acc:\n",
    "                    os.makedirs(config.checkpoints, exist_ok=True)\n",
    "                    save_path = os.path.join(config.checkpoints,'epoch-resnet50-%d-acc-%.2f.pth' % (epoch + 1, acc * 100))\n",
    "                    self.save_model(save_path)\n",
    "                    print('%s saved successfully...' % save_path)\n",
    "                    self.best_acc = acc\n",
    "                    self.best_checkpoint_path = save_path\n",
    "\n",
    "    def train_epoch(self, epoch):\n",
    "        total_loss = 0\n",
    "        corrects = 0\n",
    "        tbar = tqdm(self.train_loader)\n",
    "        self.model.train()\n",
    "        for i, (img, label) in enumerate(tbar):\n",
    "            img = img.to(self.device)\n",
    "            label = label.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            pred = self.model(img)\n",
    "            loss = self.criterion(pred[0], label[:, 0]) + \\\n",
    "                   self.criterion(pred[1], label[:, 1]) + \\\n",
    "                   self.criterion(pred[2], label[:, 2]) + \\\n",
    "                   self.criterion(pred[3], label[:, 3])\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            temp = t.stack([ \\\n",
    "                pred[0].argmax(1) == label[:, 0], \\\n",
    "                pred[1].argmax(1) == label[:, 1], \\\n",
    "                pred[2].argmax(1) == label[:, 2], \\\n",
    "                pred[3].argmax(1) == label[:, 3], ], dim=1)\n",
    "            corrects += t.all(temp, dim=1).sum().item()\n",
    "            tbar.set_description(\n",
    "                'loss: %.3f, acc: %.3f' % (loss / (i + 1), corrects * 100 / ((i + 1) * config.batch_size)))\n",
    "            if (i + 1) % config.print_interval == 0:\n",
    "                self.lr_scheduler.step()\n",
    "        return corrects * 100 / ((i + 1) * config.batch_size)\n",
    "\n",
    "    def eval(self):\n",
    "        self.model.eval()\n",
    "        corrects = 0\n",
    "        with t.no_grad():\n",
    "            tbar = tqdm(self.val_loader)\n",
    "            for i, (img, label) in enumerate(tbar):\n",
    "                img = img.to(self.device)\n",
    "                label = label.to(self.device)\n",
    "                pred = self.model(img)\n",
    "                temp = t.stack([\n",
    "                    pred[0].argmax(1) == label[:, 0], \\\n",
    "                    pred[1].argmax(1) == label[:, 1], \\\n",
    "                    pred[2].argmax(1) == label[:, 2], \\\n",
    "                    pred[3].argmax(1) == label[:, 3], \\\n",
    "                    ], dim=1)\n",
    "                corrects += t.all(temp, dim=1).sum().item()\n",
    "                tbar.set_description('Val Acc: %.2f' % (corrects * 100 / ((i + 1) * config.batch_size)))\n",
    "        self.model.train()\n",
    "        return corrects / (len(self.val_loader) * config.batch_size)\n",
    "\n",
    "    def save_model(self, save_path, save_opt=False, save_config=False):\n",
    "        dicts = {}\n",
    "        dicts['model'] = self.model.state_dict()\n",
    "        if save_opt:\n",
    "            dicts['opt'] = self.optimizer.state_dict()\n",
    "        if save_config:\n",
    "            dicts['config'] = {s: config.__getattribute__(s) for s in dir(config) if not s.startswith('_')}\n",
    "        t.save(dicts, save_path)\n",
    "\n",
    "    def load_model(self, load_path, changed=False, save_opt=False, save_config=False):\n",
    "        dicts = t.load(load_path)\n",
    "        if not changed:\n",
    "            self.model.load_state_dict(dicts['model'])\n",
    "        # else:\n",
    "        #     dicts = t.load(load_path)['model']\n",
    "        #     keys = list(net.state_dict().keys())\n",
    "        #     values = list(dicts.values())\n",
    "        #     new_dicts = {k: v for k, v in zip(keys, values)}\n",
    "        #     self.model.load_state_dict(new_dicts)\n",
    "\n",
    "        if save_opt:\n",
    "            self.optimizer.load_state_dict(dicts['opt'])\n",
    "\n",
    "        if save_config:\n",
    "            for k, v in dicts['config'].items():\n",
    "                config.__setattr__(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse2class(prediction):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "    prediction(tuple of tensor): \n",
    "    \"\"\"\n",
    "    ch1, ch2, ch3, ch4 = prediction\n",
    "    char_list = [str(i) for i in range(10)]\n",
    "    char_list.append('')\n",
    "    ch1, ch2, ch3, ch4 = ch1.argmax(1), ch2.argmax(1), ch3.argmax(1), ch4.argmax(1)\n",
    "    ch1, ch2, ch3, ch4 = [char_list[i.item()] for i in ch1], [char_list[i.item()] for i in ch2], \\\n",
    "                    [char_list[i.item()] for i in ch3], [char_list[i.item()] for i in ch4] \n",
    "    res = [c1+c2+c3+c4 for c1, c2, c3, c4 in zip(ch1, ch2, ch3, ch4)]             \n",
    "    return res\n",
    "\n",
    "def write2csv(results,csv_path):\n",
    "    \"\"\"\n",
    "    results(list):\n",
    "    \"\"\"\n",
    "    # 定义输出文件\n",
    "    df = pd.DataFrame(results, columns=['file_name', 'file_code'])\n",
    "    df['file_name'] = df['file_name'].apply(lambda x: x.split('/')[-1])\n",
    "    save_name = csv_path\n",
    "    df.to_csv(save_name, sep=',', index=None)\n",
    "    print('Results.saved to %s'%save_name)\n",
    "\n",
    "def predicts(model_path,csv_path):\n",
    "    test_loader = DataLoader(DigitsDataset(mode='test', aug=False), batch_size=config.batch_size, shuffle=False,\\\n",
    "                    num_workers=8, pin_memory=True, drop_last=False)\n",
    "    results = []\n",
    "    res_path = model_path\n",
    "    res_net = DigitsResnet50().cuda()\n",
    "    res_net.load_state_dict(t.load(res_path)['model'])\n",
    "    print('Load model from %s successfully'%model_path)\n",
    "    tbar = tqdm(test_loader)\n",
    "    res_net.eval()\n",
    "    with t.no_grad():\n",
    "        for i, (img, img_names) in enumerate(tbar):\n",
    "            img = img.cuda()\n",
    "            pred = res_net(img)\n",
    "            results += [[name, code] for name, code in zip(img_names, parse2class(pred))]\n",
    "    results = sorted(results, key=lambda x: x[0])\n",
    "    write2csv(results,csv_path)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts(trainer.best_checkpoint_path,\"result.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpx_clip4str1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
